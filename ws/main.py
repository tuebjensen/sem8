# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.


import argparse
import dataclasses
import os
import pickle
import time

import numpy as np
import torch
from torch.distributed import init_process_group
from torch.nn.parallel import DistributedDataParallel as DDP

import original_MRQ.env_preprocessing as env_preprocessing
import original_MRQ.MRQ as MRQ
import original_MRQ.utils as utils


@dataclasses.dataclass
class DefaultExperimentArguments:
    Atari_total_timesteps: int = 25e5
    Atari_eval_freq: int = 1e5

    Dmc_total_timesteps: int = 5e5
    Dmc_eval_freq: int = 5e3

    Gym_total_timesteps: int = 1e6  # 1e6
    Gym_eval_freq: int = 5e3

    Frozen_total_timesteps: int = 7e5  # 1e6
    Frozen_eval_freq: int = 5e3

    def __post_init__(self):
        utils.enforce_dataclass_type(self)


def ddp_setup() -> int:
    init_process_group(backend="nccl")
    device = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(device)
    return device


def main(args):
    device = ddp_setup()

    default_arguments = DefaultExperimentArguments()
    env_type = args.env.split("-", 1)[0]
    if args.total_timesteps == -1:
        args.total_timesteps = default_arguments.__dict__[f"{env_type}_total_timesteps"]
    if args.eval_freq == -1:
        args.eval_freq = default_arguments.__dict__[f"{env_type}_eval_freq"]

    # File name and make folders
    if args.project_name == "":
        args.project_name = f"MRQ+{args.env}+{args.seed}"
    if not os.path.exists(args.eval_folder):
        os.makedirs(args.eval_folder)
    if not os.path.exists(args.log_folder):
        os.makedirs(args.log_folder)
    if args.save_experiment and not os.path.exists(
        f"{args.save_folder}/{args.project_name}"
    ):
        os.makedirs(f"{args.save_folder}/{args.project_name}")

    np.random.seed(args.seed)
    torch.manual_seed(args.seed)

    if args.load_experiment:
        exp = load_experiment(args.save_folder, args.project_name, device, args)
    else:
        env = env_preprocessing.Env(args.env, args.seed, eval_env=False)
        eval_env = env_preprocessing.Env(
            args.env, args.seed + 100, eval_env=True
        )  # +100 to make sure the seed is different.

        agent = MRQ.Agent(
            env.obs_shape,
            env.action_dim,
            env.max_action,
            env.pixel_obs,
            env.discrete,
            device,
            env.history,
            DDP=DDP,
        )

        logger = utils.Logger(f"{args.log_folder}/{args.project_name}.txt")

        exp = OnlineExperiment(
            agent,
            env,
            eval_env,
            logger,
            [],
            0,
            args.total_timesteps,
            0,
            args.eval_freq,
            args.eval_eps,
            args.eval_folder,
            args.project_name,
            args.save_experiment,
            args.save_freq,
            args.save_folder,
        )

    exp.logger.title("Experiment")
    exp.logger.log_print(f"Algorithm:\t{exp.agent.name}")
    exp.logger.log_print(f"Env:\t\t{exp.env.env_name}")
    exp.logger.log_print(f"Seed:\t\t{exp.env.seed}")

    exp.logger.title("Environment hyperparameters")
    if hasattr(exp.env.env, "hp"):
        exp.logger.log_print(exp.env.env.hp)
    exp.logger.log_print(f"Obs shape:\t\t{exp.env.obs_shape}")
    exp.logger.log_print(f"Action dim:\t\t{exp.env.action_dim}")
    exp.logger.log_print(f"Discrete actions:\t{exp.env.discrete}")
    exp.logger.log_print(f"Pixel observations:\t{exp.env.pixel_obs}")

    exp.logger.title("Agent hyperparameters")
    exp.logger.log_print(exp.agent.hp)
    exp.logger.log_print("-" * 40)

    exp.run()


class OnlineExperiment:
    def __init__(
        self,
        agent: MRQ.Agent,
        env: object,
        eval_env: object,
        logger: object,
        evals: list,
        t: int,
        total_timesteps: int,
        time_passed: float,
        eval_freq: int,
        eval_eps: int,
        eval_folder: str,
        project_name: str,
        save_full: bool = False,
        save_freq: int = 1e5,
        save_folder: str = "",
    ):
        self.agent = agent
        self.env = env
        self.eval_env = eval_env
        self.evals = evals

        self.logger = logger

        self.t = t
        self.time_passed = time_passed
        self.start_time = time.time()

        self.total_timesteps = total_timesteps
        self.eval_freq = eval_freq
        self.eval_eps = eval_eps

        self.eval_folder = eval_folder
        self.project_name = project_name
        self.save_full = save_full
        self.save_freq = save_freq
        self.save_folder = save_folder

        self.init_timestep = True

    def run(self):
        state = self.env.reset()
        while self.t <= self.total_timesteps:
            self.maybe_evaluate()
            if (
                self.save_full
                and self.t % self.save_freq == 0
                and not self.init_timestep
            ):
                save_experiment(self)

            action = self.agent.select_action(state)
            if action is None:
                action = self.env.action_space.sample()

            next_state, reward, terminated, truncated = self.env.step(action)
            self.agent.replay_buffer.add(
                state, action, next_state, reward, terminated, truncated
            )
            state = next_state

            self.agent.train()

            if terminated or truncated:
                self.logger.log_print(
                    f"Total T: {self.t + 1}, "
                    f"Episode Num: {self.env.ep_num}, "
                    f"Episode T: {self.env.ep_timesteps}, "
                    f"Reward: {self.env.ep_total_reward:.3f}"
                )
                state = self.env.reset()

            self.t += 1
            self.init_timestep = False

    def maybe_evaluate(self):
        if self.t % self.eval_freq != 0:
            return

        # We save after evaluating, this avoids re-evaluating immediately after loading an experiment.
        if self.t != 0 and self.init_timestep:
            return

        total_reward = np.zeros(self.eval_eps)
        for ep in range(self.eval_eps):
            state, terminated, truncated = self.eval_env.reset(), False, False
            while not (terminated or truncated):
                action = self.agent.select_action(
                    np.array(state), use_exploration=False
                )
                state, _, terminated, truncated = self.eval_env.step(action)
            total_reward[ep] = self.eval_env.ep_total_reward

        self.evals.append(total_reward.mean())

        self.logger.title(
            f"Evaluation at {self.t} time steps\n"
            f"Average total reward over {self.eval_eps} episodes: {total_reward.mean():.3f}\n"
            f"Total time passed: {round((time.time() - self.start_time + self.time_passed) / 60.0, 2)} minutes"
        )

        np.savetxt(
            f"{self.eval_folder}/{self.project_name}.txt", self.evals, fmt="%.14f"
        )


def save_experiment(exp: OnlineExperiment):
    # Save experiment settings
    exp.time_passed += time.time() - exp.start_time
    var_dict = {k: exp.__dict__[k] for k in ["t", "eval_freq", "eval_eps"]}
    var_dict["time_passed"] = exp.time_passed + time.time() - exp.start_time
    var_dict["np_seed"] = np.random.get_state()
    var_dict["torch_seed"] = torch.get_rng_state()
    np.save(f"{exp.save_folder}/{exp.project_name}/exp_var.npy", var_dict)
    # Save eval
    np.savetxt(f"{exp.save_folder}/{exp.project_name}.txt", exp.evals, fmt="%.14f")
    # Save envs
    pickle.dump(
        exp.env, file=open(f"{exp.save_folder}/{exp.project_name}/env.pickle", "wb")
    )
    pickle.dump(
        exp.eval_env,
        file=open(f"{exp.save_folder}/{exp.project_name}/eval_env.pickle", "wb"),
    )
    # Save agent
    exp.agent.save(f"{exp.save_folder}/{exp.project_name}")

    exp.logger.title("Saved experiment")


def load_experiment(
    save_folder: str, project_name: str, device: torch.device, args: object
):
    # Load experiment settings
    exp_dict = np.load(
        f"{save_folder}/{project_name}/exp_var.npy", allow_pickle=True
    ).item()
    # This is not sufficient to guarantee the experiment will run exactly the same,
    # however, it does mean the original seed is not reused.
    np.random.set_state(exp_dict["np_seed"])
    torch.set_rng_state(exp_dict["torch_seed"])
    # Load eval
    evals = np.loadtxt(f"{save_folder}/{project_name}.txt").tolist()
    # Load envs
    env = pickle.load(open(f"{save_folder}/{project_name}/env.pickle", "rb"))
    eval_env = pickle.load(open(f"{save_folder}/{project_name}/eval_env.pickle", "rb"))
    # Load agent
    agent_dict = np.load(
        f"{save_folder}/{project_name}/agent_var.npy", allow_pickle=True
    ).item()
    agent = MRQ.Agent(
        env.obs_shape,
        env.action_dim,
        env.max_action,
        env.pixel_obs,
        env.discrete,
        device,
        env.history,
        dataclasses.asdict(agent_dict["hp"]),
        DDP=DDP,
    )
    agent.load(f"{save_folder}/{project_name}")

    logger = utils.Logger(f"{args.log_folder}/{args.project_name}.txt")
    logger.title(f"Loaded experiment\nStarting from: {exp_dict['t']} time steps.")

    return OnlineExperiment(
        agent,
        env,
        eval_env,
        logger,
        evals,
        exp_dict["t"],
        args.total_timesteps,
        exp_dict["time_passed"],
        exp_dict["eval_freq"],
        exp_dict["eval_eps"],
        args.eval_folder,
        args.project_name,
        args.save_experiment,
        args.save_freq,
        args.save_folder,
    )


def modify_parser(parser):
    # Experiment
    parser.add_argument("--env", default="Gym-HalfCheetah-v4", type=str)
    parser.add_argument("--seed", default=0, type=int)
    parser.add_argument(
        "--total_timesteps", default=-1, type=int
    )  # Uses default, input to override.
    parser.add_argument("--device", default="cuda", type=str)
    # Evaluation
    parser.add_argument(
        "--eval_freq", default=-1, type=int
    )  # Uses default, input to override.
    parser.add_argument("--eval_eps", default=10, type=int)
    # File name and locations
    parser.add_argument(
        "--project_name", default="", type=str
    )  # Uses default, input to override.
    parser.add_argument("--eval_folder", default="./evals", type=str)
    parser.add_argument("--log_folder", default="./logs", type=str)
    parser.add_argument("--save_folder", default="./checkpoint", type=str)
    # Experiment checkpointing
    parser.add_argument(
        "--save_experiment",
        default=False,
        action=argparse.BooleanOptionalAction,
        type=bool,
    )
    parser.add_argument("--save_freq", default=1e5, type=int)
    parser.add_argument(
        "--load_experiment",
        default=False,
        action=argparse.BooleanOptionalAction,
        type=bool,
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    modify_parser(parser)
    args = parser.parse_args()
    main(args)
